{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow实现进阶的卷积神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cifar10 # 将下载的TensorFlow Models库里的cifar10放到项目中\n",
    "import cifar10_input # 将cifar文件夹中的cifar10_input.py复制到外层文件夹中\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练轮数max_steps\n",
    "max_steps = 3000\n",
    "# 定义batch_size\n",
    "batch_size = 128\n",
    "# 定义下载CIFAR-10数据的路径\n",
    "data_dir = 'temp/cifar10_data/cifar-10-batches-bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_with_weight_loss(shape, stddev, wl):\n",
    "    var = tf.Variable(tf.truncated_normal(shape, stddev=stddev))\n",
    "    if wl is not None:\n",
    "        weight_loss = tf.multiply(tf.nn.l2_loss(var), wl, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_loss)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n"
     ]
    }
   ],
   "source": [
    "# cifar10.maybe_download_and_extract() # 'module' object has no attribute 'maybe_download_and_extract'\n",
    "# download_url: http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n",
    "# 准备训练数据\n",
    "images_train, labels_train = cifar10_input.distorted_inputs(data_dir=data_dir, batch_size=batch_size)\n",
    "# 准备测试数据\n",
    "images_test, labels_test = cifar10_input.inputs(eval_data=True, data_dir=data_dir, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建输入数据的placeholder\n",
    "image_holder = tf.placeholder(tf.float32, [batch_size, 24, 24, 3])\n",
    "label_holder = tf.placeholder(tf.int32, [batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建第一个卷积层\n",
    "weight1 = variable_with_weight_loss(shape=[5, 5, 3, 64], stddev=5e-2, wl=0.0)\n",
    "kernel1 = tf.nn.conv2d(image_holder, weight1, [1, 1, 1, 1], padding='SAME')\n",
    "bias1 = tf.Variable(tf.constant(0.0, shape=[64]))\n",
    "conv1 = tf.nn.relu(tf.nn.bias_add(kernel1, bias1))\n",
    "pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建第二个卷积层\n",
    "weight2 = variable_with_weight_loss(shape=[5, 5, 64, 64], stddev=5e-2, wl=0.0)\n",
    "kernel2 = tf.nn.conv2d(norm1, weight2, [1, 1, 1, 1], padding='SAME')\n",
    "bias2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "conv2 = tf.nn.relu(tf.nn.bias_add(kernel2, bias2))\n",
    "norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建全连接层\n",
    "reshape = tf.reshape(pool2, [batch_size, -1])\n",
    "dim = reshape.get_shape()[1].value\n",
    "weight3 = variable_with_weight_loss(shape=[dim, 384], stddev=0.04, wl=0.004)\n",
    "bias3 = tf.Variable(tf.constant(0.1, shape=[384]))\n",
    "local3 = tf.nn.relu(tf.matmul(reshape, weight3) + bias3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 类似全连接层，隐含节点数下降一半\n",
    "weight4 = variable_with_weight_loss(shape=[384, 192], stddev=0.04, wl=0.004)\n",
    "bias4 = tf.Variable(tf.constant(0.1, shape=[192]))\n",
    "local4 = tf.nn.relu(tf.matmul(local3, weight4) + bias4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最后一层\n",
    "weight5 = variable_with_weight_loss(shape=[192, 10], stddev=1/192, wl=0.0)\n",
    "bias5 = tf.Variable(tf.constant(0.0, shape=[10]))\n",
    "logits = tf.add(tf.matmul(local4, weight5), bias5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(logits, labels):\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels, name='cross_entropy_per_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)\n",
    "    return tf.add_n(tf.get_collection('losses'), name='total_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss(logits, label_holder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择Adam优化器，设定学习速率为1e-3\n",
    "train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出Top k\n",
    "top_k_op = tf.nn.in_top_k(logits, label_holder, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建session\n",
    "session = tf.InteractiveSession()\n",
    "# 变量全局初始化\n",
    "\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Thread(QueueRunnerThread-input_producer-input_producer/input_producer_EnqueueMany, started daemon 123145341480960)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 123145345687552)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 123145349894144)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 123145354100736)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 123145358307328)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 123145362513920)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 123145366720512)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 123145370927104)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 123145375133696)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 123145379340288)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 123145383546880)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 123145387753472)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 123145391960064)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 123145396166656)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 123145400373248)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 123145404579840)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 123145408786432)>,\n",
       " <Thread(QueueRunnerThread-input_producer_1-input_producer_1/input_producer_1_EnqueueMany, started daemon 123145412993024)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 123145417199616)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 123145421406208)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 123145425612800)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 123145429819392)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 123145434025984)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 123145438232576)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 123145442439168)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 123145446645760)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 123145450852352)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 123145455058944)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 123145459265536)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 123145463472128)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 123145467678720)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 123145471885312)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 123145476091904)>,\n",
       " <Thread(QueueRunnerThread-batch/fifo_queue-batch/fifo_queue_enqueue, started daemon 123145480298496)>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 启动图片数据增强线程队列\n",
    "tf.train.start_queue_runners()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss=4.67 (29.7 examples/sec; 4.305 sec/batch)\n",
      "step 10, loss=3.62 (189.3 examples/sec; 0.676 sec/batch)\n",
      "step 20, loss=3.15 (158.5 examples/sec; 0.808 sec/batch)\n",
      "step 30, loss=2.68 (209.3 examples/sec; 0.612 sec/batch)\n",
      "step 40, loss=2.62 (230.6 examples/sec; 0.555 sec/batch)\n",
      "step 50, loss=2.33 (262.4 examples/sec; 0.488 sec/batch)\n",
      "step 60, loss=2.24 (172.4 examples/sec; 0.743 sec/batch)\n",
      "step 70, loss=2.19 (209.9 examples/sec; 0.610 sec/batch)\n",
      "step 80, loss=2.06 (203.3 examples/sec; 0.630 sec/batch)\n",
      "step 90, loss=2.03 (250.0 examples/sec; 0.512 sec/batch)\n",
      "step 100, loss=1.90 (232.4 examples/sec; 0.551 sec/batch)\n",
      "step 110, loss=1.88 (184.8 examples/sec; 0.693 sec/batch)\n",
      "step 120, loss=2.04 (241.4 examples/sec; 0.530 sec/batch)\n",
      "step 130, loss=1.89 (229.4 examples/sec; 0.558 sec/batch)\n",
      "step 140, loss=1.81 (225.2 examples/sec; 0.568 sec/batch)\n",
      "step 150, loss=1.93 (222.9 examples/sec; 0.574 sec/batch)\n",
      "step 160, loss=1.94 (228.8 examples/sec; 0.560 sec/batch)\n",
      "step 170, loss=1.77 (273.1 examples/sec; 0.469 sec/batch)\n",
      "step 180, loss=1.81 (238.6 examples/sec; 0.537 sec/batch)\n",
      "step 190, loss=1.75 (260.4 examples/sec; 0.491 sec/batch)\n",
      "step 200, loss=1.73 (276.8 examples/sec; 0.462 sec/batch)\n",
      "step 210, loss=1.72 (224.2 examples/sec; 0.571 sec/batch)\n",
      "step 220, loss=1.82 (251.6 examples/sec; 0.509 sec/batch)\n",
      "step 230, loss=1.69 (275.1 examples/sec; 0.465 sec/batch)\n",
      "step 240, loss=1.69 (230.5 examples/sec; 0.555 sec/batch)\n",
      "step 250, loss=1.68 (240.1 examples/sec; 0.533 sec/batch)\n",
      "step 260, loss=1.44 (261.2 examples/sec; 0.490 sec/batch)\n",
      "step 270, loss=1.69 (222.5 examples/sec; 0.575 sec/batch)\n",
      "step 280, loss=1.58 (233.7 examples/sec; 0.548 sec/batch)\n",
      "step 290, loss=1.65 (273.1 examples/sec; 0.469 sec/batch)\n",
      "step 300, loss=1.67 (256.2 examples/sec; 0.500 sec/batch)\n",
      "step 310, loss=1.68 (278.8 examples/sec; 0.459 sec/batch)\n",
      "step 320, loss=1.50 (276.0 examples/sec; 0.464 sec/batch)\n",
      "step 330, loss=1.60 (246.8 examples/sec; 0.519 sec/batch)\n",
      "step 340, loss=1.44 (241.9 examples/sec; 0.529 sec/batch)\n",
      "step 350, loss=1.49 (255.9 examples/sec; 0.500 sec/batch)\n",
      "step 360, loss=1.55 (268.0 examples/sec; 0.478 sec/batch)\n",
      "step 370, loss=1.40 (240.5 examples/sec; 0.532 sec/batch)\n",
      "step 380, loss=1.61 (242.8 examples/sec; 0.527 sec/batch)\n",
      "step 390, loss=1.74 (235.9 examples/sec; 0.543 sec/batch)\n",
      "step 400, loss=1.45 (252.0 examples/sec; 0.508 sec/batch)\n",
      "step 410, loss=1.30 (276.7 examples/sec; 0.463 sec/batch)\n",
      "step 420, loss=1.42 (276.1 examples/sec; 0.464 sec/batch)\n",
      "step 430, loss=1.50 (256.6 examples/sec; 0.499 sec/batch)\n",
      "step 440, loss=1.56 (260.4 examples/sec; 0.492 sec/batch)\n",
      "step 450, loss=1.51 (268.2 examples/sec; 0.477 sec/batch)\n",
      "step 460, loss=1.55 (270.1 examples/sec; 0.474 sec/batch)\n",
      "step 470, loss=1.42 (239.3 examples/sec; 0.535 sec/batch)\n",
      "step 480, loss=1.68 (271.3 examples/sec; 0.472 sec/batch)\n",
      "step 490, loss=1.41 (252.7 examples/sec; 0.506 sec/batch)\n",
      "step 500, loss=1.46 (244.4 examples/sec; 0.524 sec/batch)\n",
      "step 510, loss=1.45 (224.2 examples/sec; 0.571 sec/batch)\n",
      "step 520, loss=1.37 (241.8 examples/sec; 0.529 sec/batch)\n",
      "step 530, loss=1.51 (208.9 examples/sec; 0.613 sec/batch)\n",
      "step 540, loss=1.49 (146.7 examples/sec; 0.873 sec/batch)\n",
      "step 550, loss=1.44 (202.1 examples/sec; 0.633 sec/batch)\n",
      "step 560, loss=1.62 (88.1 examples/sec; 1.453 sec/batch)\n",
      "step 570, loss=1.44 (143.0 examples/sec; 0.895 sec/batch)\n",
      "step 580, loss=1.55 (180.1 examples/sec; 0.711 sec/batch)\n",
      "step 590, loss=1.28 (248.2 examples/sec; 0.516 sec/batch)\n",
      "step 600, loss=1.41 (221.9 examples/sec; 0.577 sec/batch)\n",
      "step 610, loss=1.37 (227.1 examples/sec; 0.564 sec/batch)\n",
      "step 620, loss=1.55 (153.8 examples/sec; 0.832 sec/batch)\n",
      "step 630, loss=1.52 (141.4 examples/sec; 0.905 sec/batch)\n",
      "step 640, loss=1.40 (140.9 examples/sec; 0.908 sec/batch)\n",
      "step 650, loss=1.30 (139.7 examples/sec; 0.916 sec/batch)\n",
      "step 660, loss=1.36 (139.3 examples/sec; 0.919 sec/batch)\n",
      "step 670, loss=1.40 (273.3 examples/sec; 0.468 sec/batch)\n",
      "step 680, loss=1.41 (189.3 examples/sec; 0.676 sec/batch)\n",
      "step 690, loss=1.41 (233.6 examples/sec; 0.548 sec/batch)\n",
      "step 700, loss=1.25 (210.7 examples/sec; 0.608 sec/batch)\n",
      "step 710, loss=1.44 (231.3 examples/sec; 0.553 sec/batch)\n",
      "step 720, loss=1.41 (186.5 examples/sec; 0.686 sec/batch)\n",
      "step 730, loss=1.15 (210.6 examples/sec; 0.608 sec/batch)\n",
      "step 740, loss=1.22 (175.9 examples/sec; 0.728 sec/batch)\n",
      "step 750, loss=1.37 (154.3 examples/sec; 0.830 sec/batch)\n",
      "step 760, loss=1.21 (211.8 examples/sec; 0.604 sec/batch)\n",
      "step 770, loss=1.22 (279.8 examples/sec; 0.457 sec/batch)\n",
      "step 780, loss=1.59 (184.3 examples/sec; 0.694 sec/batch)\n",
      "step 790, loss=1.36 (209.8 examples/sec; 0.610 sec/batch)\n",
      "step 800, loss=1.35 (250.4 examples/sec; 0.511 sec/batch)\n",
      "step 810, loss=1.21 (74.8 examples/sec; 1.710 sec/batch)\n",
      "step 820, loss=1.33 (288.2 examples/sec; 0.444 sec/batch)\n",
      "step 830, loss=1.32 (247.8 examples/sec; 0.517 sec/batch)\n",
      "step 840, loss=1.20 (218.8 examples/sec; 0.585 sec/batch)\n",
      "step 850, loss=1.41 (285.0 examples/sec; 0.449 sec/batch)\n",
      "step 860, loss=1.44 (234.9 examples/sec; 0.545 sec/batch)\n",
      "step 870, loss=1.25 (239.1 examples/sec; 0.535 sec/batch)\n",
      "step 880, loss=1.15 (220.4 examples/sec; 0.581 sec/batch)\n",
      "step 890, loss=1.22 (291.8 examples/sec; 0.439 sec/batch)\n",
      "step 900, loss=1.32 (199.2 examples/sec; 0.642 sec/batch)\n",
      "step 910, loss=1.40 (253.5 examples/sec; 0.505 sec/batch)\n",
      "step 920, loss=1.32 (272.9 examples/sec; 0.469 sec/batch)\n",
      "step 930, loss=1.38 (254.3 examples/sec; 0.503 sec/batch)\n",
      "step 940, loss=1.29 (280.5 examples/sec; 0.456 sec/batch)\n",
      "step 950, loss=1.21 (282.0 examples/sec; 0.454 sec/batch)\n",
      "step 960, loss=1.23 (155.8 examples/sec; 0.821 sec/batch)\n",
      "step 970, loss=1.18 (283.0 examples/sec; 0.452 sec/batch)\n",
      "step 980, loss=1.31 (283.3 examples/sec; 0.452 sec/batch)\n",
      "step 990, loss=1.24 (213.5 examples/sec; 0.599 sec/batch)\n",
      "step 1000, loss=1.41 (259.1 examples/sec; 0.494 sec/batch)\n",
      "step 1010, loss=1.18 (236.2 examples/sec; 0.542 sec/batch)\n",
      "step 1020, loss=1.21 (226.0 examples/sec; 0.566 sec/batch)\n",
      "step 1030, loss=1.22 (263.3 examples/sec; 0.486 sec/batch)\n",
      "step 1040, loss=1.34 (202.3 examples/sec; 0.633 sec/batch)\n",
      "step 1050, loss=1.29 (106.1 examples/sec; 1.207 sec/batch)\n",
      "step 1060, loss=1.32 (239.8 examples/sec; 0.534 sec/batch)\n",
      "step 1070, loss=1.30 (233.3 examples/sec; 0.549 sec/batch)\n",
      "step 1080, loss=1.18 (242.1 examples/sec; 0.529 sec/batch)\n",
      "step 1090, loss=1.17 (243.6 examples/sec; 0.526 sec/batch)\n",
      "step 1100, loss=1.34 (269.2 examples/sec; 0.476 sec/batch)\n",
      "step 1110, loss=1.33 (274.0 examples/sec; 0.467 sec/batch)\n",
      "step 1120, loss=1.27 (276.4 examples/sec; 0.463 sec/batch)\n",
      "step 1130, loss=1.19 (240.8 examples/sec; 0.532 sec/batch)\n",
      "step 1140, loss=1.18 (231.7 examples/sec; 0.553 sec/batch)\n",
      "step 1150, loss=1.37 (222.6 examples/sec; 0.575 sec/batch)\n",
      "step 1160, loss=1.35 (263.3 examples/sec; 0.486 sec/batch)\n",
      "step 1170, loss=1.29 (270.0 examples/sec; 0.474 sec/batch)\n",
      "step 1180, loss=1.30 (233.4 examples/sec; 0.548 sec/batch)\n",
      "step 1190, loss=1.23 (240.6 examples/sec; 0.532 sec/batch)\n",
      "step 1200, loss=1.32 (266.1 examples/sec; 0.481 sec/batch)\n",
      "step 1210, loss=1.12 (283.9 examples/sec; 0.451 sec/batch)\n",
      "step 1220, loss=1.26 (278.7 examples/sec; 0.459 sec/batch)\n",
      "step 1230, loss=1.03 (243.4 examples/sec; 0.526 sec/batch)\n",
      "step 1240, loss=1.22 (237.6 examples/sec; 0.539 sec/batch)\n",
      "step 1250, loss=1.15 (270.8 examples/sec; 0.473 sec/batch)\n",
      "step 1260, loss=1.21 (157.5 examples/sec; 0.813 sec/batch)\n",
      "step 1270, loss=1.12 (232.6 examples/sec; 0.550 sec/batch)\n",
      "step 1280, loss=1.23 (168.8 examples/sec; 0.758 sec/batch)\n",
      "step 1290, loss=1.25 (183.5 examples/sec; 0.698 sec/batch)\n",
      "step 1300, loss=1.09 (266.9 examples/sec; 0.480 sec/batch)\n",
      "step 1310, loss=1.19 (240.1 examples/sec; 0.533 sec/batch)\n",
      "step 1320, loss=1.31 (233.7 examples/sec; 0.548 sec/batch)\n",
      "step 1330, loss=1.36 (234.2 examples/sec; 0.546 sec/batch)\n",
      "step 1340, loss=1.10 (223.2 examples/sec; 0.573 sec/batch)\n",
      "step 1350, loss=0.97 (248.5 examples/sec; 0.515 sec/batch)\n",
      "step 1360, loss=1.11 (285.1 examples/sec; 0.449 sec/batch)\n",
      "step 1370, loss=1.13 (264.9 examples/sec; 0.483 sec/batch)\n",
      "step 1380, loss=1.21 (277.8 examples/sec; 0.461 sec/batch)\n",
      "step 1390, loss=1.19 (237.4 examples/sec; 0.539 sec/batch)\n",
      "step 1400, loss=1.29 (235.4 examples/sec; 0.544 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1410, loss=1.06 (259.0 examples/sec; 0.494 sec/batch)\n",
      "step 1420, loss=1.08 (266.8 examples/sec; 0.480 sec/batch)\n",
      "step 1430, loss=1.33 (205.2 examples/sec; 0.624 sec/batch)\n",
      "step 1440, loss=1.18 (197.1 examples/sec; 0.649 sec/batch)\n",
      "step 1450, loss=1.04 (198.6 examples/sec; 0.645 sec/batch)\n",
      "step 1460, loss=1.30 (236.5 examples/sec; 0.541 sec/batch)\n",
      "step 1470, loss=1.29 (264.3 examples/sec; 0.484 sec/batch)\n",
      "step 1480, loss=1.28 (258.9 examples/sec; 0.494 sec/batch)\n",
      "step 1490, loss=1.31 (191.3 examples/sec; 0.669 sec/batch)\n",
      "step 1500, loss=1.29 (244.5 examples/sec; 0.523 sec/batch)\n",
      "step 1510, loss=1.11 (167.5 examples/sec; 0.764 sec/batch)\n",
      "step 1520, loss=1.24 (201.6 examples/sec; 0.635 sec/batch)\n",
      "step 1530, loss=1.12 (136.5 examples/sec; 0.938 sec/batch)\n",
      "step 1540, loss=1.23 (231.4 examples/sec; 0.553 sec/batch)\n",
      "step 1550, loss=1.24 (267.9 examples/sec; 0.478 sec/batch)\n",
      "step 1560, loss=1.05 (310.6 examples/sec; 0.412 sec/batch)\n",
      "step 1570, loss=1.22 (312.0 examples/sec; 0.410 sec/batch)\n",
      "step 1580, loss=1.23 (309.7 examples/sec; 0.413 sec/batch)\n",
      "step 1590, loss=0.99 (285.8 examples/sec; 0.448 sec/batch)\n",
      "step 1600, loss=1.20 (295.9 examples/sec; 0.433 sec/batch)\n",
      "step 1610, loss=1.18 (310.3 examples/sec; 0.413 sec/batch)\n",
      "step 1620, loss=0.98 (56.6 examples/sec; 2.261 sec/batch)\n",
      "step 1630, loss=1.15 (220.8 examples/sec; 0.580 sec/batch)\n",
      "step 1640, loss=1.10 (183.5 examples/sec; 0.698 sec/batch)\n",
      "step 1650, loss=1.09 (219.8 examples/sec; 0.582 sec/batch)\n",
      "step 1660, loss=1.23 (198.1 examples/sec; 0.646 sec/batch)\n",
      "step 1670, loss=1.38 (255.2 examples/sec; 0.502 sec/batch)\n",
      "step 1680, loss=1.25 (247.7 examples/sec; 0.517 sec/batch)\n",
      "step 1690, loss=1.07 (267.6 examples/sec; 0.478 sec/batch)\n",
      "step 1700, loss=1.28 (273.7 examples/sec; 0.468 sec/batch)\n",
      "step 1710, loss=1.02 (223.0 examples/sec; 0.574 sec/batch)\n",
      "step 1720, loss=1.22 (244.4 examples/sec; 0.524 sec/batch)\n",
      "step 1730, loss=1.22 (256.7 examples/sec; 0.499 sec/batch)\n",
      "step 1740, loss=1.00 (262.5 examples/sec; 0.488 sec/batch)\n",
      "step 1750, loss=1.12 (230.3 examples/sec; 0.556 sec/batch)\n",
      "step 1760, loss=1.08 (245.0 examples/sec; 0.522 sec/batch)\n",
      "step 1770, loss=1.31 (193.6 examples/sec; 0.661 sec/batch)\n",
      "step 1780, loss=1.15 (194.2 examples/sec; 0.659 sec/batch)\n",
      "step 1790, loss=0.99 (213.3 examples/sec; 0.600 sec/batch)\n",
      "step 1800, loss=1.08 (247.4 examples/sec; 0.517 sec/batch)\n",
      "step 1810, loss=0.99 (143.6 examples/sec; 0.891 sec/batch)\n",
      "step 1820, loss=1.21 (122.8 examples/sec; 1.043 sec/batch)\n",
      "step 1830, loss=1.21 (249.7 examples/sec; 0.513 sec/batch)\n",
      "step 1840, loss=0.94 (232.2 examples/sec; 0.551 sec/batch)\n",
      "step 1850, loss=1.15 (288.2 examples/sec; 0.444 sec/batch)\n",
      "step 1860, loss=1.14 (253.7 examples/sec; 0.505 sec/batch)\n",
      "step 1870, loss=1.10 (293.4 examples/sec; 0.436 sec/batch)\n",
      "step 1880, loss=1.03 (245.4 examples/sec; 0.522 sec/batch)\n",
      "step 1890, loss=1.10 (193.7 examples/sec; 0.661 sec/batch)\n",
      "step 1900, loss=1.03 (240.0 examples/sec; 0.533 sec/batch)\n",
      "step 1910, loss=1.32 (164.3 examples/sec; 0.779 sec/batch)\n",
      "step 1920, loss=1.18 (239.5 examples/sec; 0.534 sec/batch)\n",
      "step 1930, loss=1.00 (275.7 examples/sec; 0.464 sec/batch)\n",
      "step 1940, loss=1.07 (276.1 examples/sec; 0.464 sec/batch)\n",
      "step 1950, loss=1.12 (260.9 examples/sec; 0.491 sec/batch)\n",
      "step 1960, loss=1.18 (287.9 examples/sec; 0.445 sec/batch)\n",
      "step 1970, loss=1.12 (270.4 examples/sec; 0.473 sec/batch)\n",
      "step 1980, loss=1.07 (234.0 examples/sec; 0.547 sec/batch)\n",
      "step 1990, loss=1.25 (268.1 examples/sec; 0.477 sec/batch)\n",
      "step 2000, loss=1.05 (266.0 examples/sec; 0.481 sec/batch)\n",
      "step 2010, loss=1.11 (269.2 examples/sec; 0.476 sec/batch)\n",
      "step 2020, loss=1.17 (236.2 examples/sec; 0.542 sec/batch)\n",
      "step 2030, loss=1.09 (290.7 examples/sec; 0.440 sec/batch)\n",
      "step 2040, loss=1.14 (286.1 examples/sec; 0.447 sec/batch)\n",
      "step 2050, loss=1.25 (280.3 examples/sec; 0.457 sec/batch)\n",
      "step 2060, loss=1.07 (275.9 examples/sec; 0.464 sec/batch)\n",
      "step 2070, loss=0.93 (257.9 examples/sec; 0.496 sec/batch)\n",
      "step 2080, loss=1.15 (265.3 examples/sec; 0.483 sec/batch)\n",
      "step 2090, loss=1.22 (192.9 examples/sec; 0.664 sec/batch)\n",
      "step 2100, loss=0.93 (244.3 examples/sec; 0.524 sec/batch)\n",
      "step 2110, loss=1.13 (247.4 examples/sec; 0.517 sec/batch)\n",
      "step 2120, loss=1.14 (226.9 examples/sec; 0.564 sec/batch)\n",
      "step 2130, loss=0.92 (221.9 examples/sec; 0.577 sec/batch)\n",
      "step 2140, loss=1.14 (257.3 examples/sec; 0.497 sec/batch)\n",
      "step 2150, loss=1.20 (286.2 examples/sec; 0.447 sec/batch)\n",
      "step 2160, loss=1.12 (201.3 examples/sec; 0.636 sec/batch)\n",
      "step 2170, loss=0.99 (229.4 examples/sec; 0.558 sec/batch)\n",
      "step 2180, loss=1.11 (285.1 examples/sec; 0.449 sec/batch)\n",
      "step 2190, loss=1.27 (210.5 examples/sec; 0.608 sec/batch)\n",
      "step 2200, loss=1.05 (271.8 examples/sec; 0.471 sec/batch)\n",
      "step 2210, loss=1.00 (285.1 examples/sec; 0.449 sec/batch)\n",
      "step 2220, loss=1.18 (278.3 examples/sec; 0.460 sec/batch)\n",
      "step 2230, loss=0.96 (237.6 examples/sec; 0.539 sec/batch)\n",
      "step 2240, loss=1.10 (277.9 examples/sec; 0.461 sec/batch)\n",
      "step 2250, loss=1.07 (277.5 examples/sec; 0.461 sec/batch)\n",
      "step 2260, loss=1.16 (280.9 examples/sec; 0.456 sec/batch)\n",
      "step 2270, loss=1.12 (268.4 examples/sec; 0.477 sec/batch)\n",
      "step 2280, loss=1.06 (242.9 examples/sec; 0.527 sec/batch)\n",
      "step 2290, loss=1.11 (257.9 examples/sec; 0.496 sec/batch)\n",
      "step 2300, loss=1.00 (264.1 examples/sec; 0.485 sec/batch)\n",
      "step 2310, loss=0.96 (272.9 examples/sec; 0.469 sec/batch)\n",
      "step 2320, loss=0.96 (288.8 examples/sec; 0.443 sec/batch)\n",
      "step 2330, loss=1.04 (241.9 examples/sec; 0.529 sec/batch)\n",
      "step 2340, loss=1.00 (275.4 examples/sec; 0.465 sec/batch)\n",
      "step 2350, loss=1.13 (269.6 examples/sec; 0.475 sec/batch)\n",
      "step 2360, loss=1.08 (279.1 examples/sec; 0.459 sec/batch)\n",
      "step 2370, loss=1.09 (249.4 examples/sec; 0.513 sec/batch)\n",
      "step 2380, loss=0.96 (263.5 examples/sec; 0.486 sec/batch)\n",
      "step 2390, loss=1.05 (271.1 examples/sec; 0.472 sec/batch)\n",
      "step 2400, loss=1.10 (285.5 examples/sec; 0.448 sec/batch)\n",
      "step 2410, loss=1.18 (268.3 examples/sec; 0.477 sec/batch)\n",
      "step 2420, loss=1.18 (265.0 examples/sec; 0.483 sec/batch)\n",
      "step 2430, loss=0.92 (244.2 examples/sec; 0.524 sec/batch)\n",
      "step 2440, loss=0.90 (264.2 examples/sec; 0.484 sec/batch)\n",
      "step 2450, loss=1.02 (277.0 examples/sec; 0.462 sec/batch)\n",
      "step 2460, loss=1.17 (270.4 examples/sec; 0.473 sec/batch)\n",
      "step 2470, loss=1.24 (273.1 examples/sec; 0.469 sec/batch)\n",
      "step 2480, loss=0.98 (244.5 examples/sec; 0.524 sec/batch)\n",
      "step 2490, loss=1.15 (271.5 examples/sec; 0.471 sec/batch)\n",
      "step 2500, loss=1.19 (260.7 examples/sec; 0.491 sec/batch)\n",
      "step 2510, loss=1.19 (288.1 examples/sec; 0.444 sec/batch)\n",
      "step 2520, loss=1.03 (271.7 examples/sec; 0.471 sec/batch)\n",
      "step 2530, loss=1.06 (230.2 examples/sec; 0.556 sec/batch)\n",
      "step 2540, loss=1.02 (270.3 examples/sec; 0.474 sec/batch)\n",
      "step 2550, loss=1.09 (283.9 examples/sec; 0.451 sec/batch)\n",
      "step 2560, loss=1.13 (271.2 examples/sec; 0.472 sec/batch)\n",
      "step 2570, loss=1.18 (269.0 examples/sec; 0.476 sec/batch)\n",
      "step 2580, loss=1.01 (260.7 examples/sec; 0.491 sec/batch)\n",
      "step 2590, loss=1.21 (265.7 examples/sec; 0.482 sec/batch)\n",
      "step 2600, loss=0.98 (281.3 examples/sec; 0.455 sec/batch)\n",
      "step 2610, loss=1.01 (273.0 examples/sec; 0.469 sec/batch)\n",
      "step 2620, loss=0.93 (253.2 examples/sec; 0.506 sec/batch)\n",
      "step 2630, loss=1.08 (258.8 examples/sec; 0.495 sec/batch)\n",
      "step 2640, loss=1.12 (277.8 examples/sec; 0.461 sec/batch)\n",
      "step 2650, loss=1.14 (277.9 examples/sec; 0.461 sec/batch)\n",
      "step 2660, loss=1.02 (283.5 examples/sec; 0.452 sec/batch)\n",
      "step 2670, loss=1.11 (271.1 examples/sec; 0.472 sec/batch)\n",
      "step 2680, loss=1.21 (269.0 examples/sec; 0.476 sec/batch)\n",
      "step 2690, loss=1.23 (273.7 examples/sec; 0.468 sec/batch)\n",
      "step 2700, loss=1.07 (282.1 examples/sec; 0.454 sec/batch)\n",
      "step 2710, loss=0.89 (271.1 examples/sec; 0.472 sec/batch)\n",
      "step 2720, loss=1.10 (260.1 examples/sec; 0.492 sec/batch)\n",
      "step 2730, loss=0.88 (253.0 examples/sec; 0.506 sec/batch)\n",
      "step 2740, loss=1.19 (280.6 examples/sec; 0.456 sec/batch)\n",
      "step 2750, loss=1.04 (272.9 examples/sec; 0.469 sec/batch)\n",
      "step 2760, loss=1.19 (285.2 examples/sec; 0.449 sec/batch)\n",
      "step 2770, loss=0.96 (226.7 examples/sec; 0.565 sec/batch)\n",
      "step 2780, loss=1.05 (236.7 examples/sec; 0.541 sec/batch)\n",
      "step 2790, loss=1.11 (278.7 examples/sec; 0.459 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2800, loss=1.07 (290.2 examples/sec; 0.441 sec/batch)\n",
      "step 2810, loss=1.17 (275.4 examples/sec; 0.465 sec/batch)\n",
      "step 2820, loss=1.07 (275.4 examples/sec; 0.465 sec/batch)\n",
      "step 2830, loss=1.15 (272.2 examples/sec; 0.470 sec/batch)\n",
      "step 2840, loss=0.92 (259.2 examples/sec; 0.494 sec/batch)\n",
      "step 2850, loss=1.39 (283.1 examples/sec; 0.452 sec/batch)\n",
      "step 2860, loss=1.05 (243.3 examples/sec; 0.526 sec/batch)\n",
      "step 2870, loss=0.88 (271.0 examples/sec; 0.472 sec/batch)\n",
      "step 2880, loss=1.02 (254.1 examples/sec; 0.504 sec/batch)\n",
      "step 2890, loss=1.14 (281.2 examples/sec; 0.455 sec/batch)\n",
      "step 2900, loss=1.11 (274.6 examples/sec; 0.466 sec/batch)\n",
      "step 2910, loss=0.84 (286.7 examples/sec; 0.446 sec/batch)\n",
      "step 2920, loss=1.06 (271.9 examples/sec; 0.471 sec/batch)\n",
      "step 2930, loss=0.90 (265.7 examples/sec; 0.482 sec/batch)\n",
      "step 2940, loss=0.85 (275.3 examples/sec; 0.465 sec/batch)\n",
      "step 2950, loss=0.94 (281.4 examples/sec; 0.455 sec/batch)\n",
      "step 2960, loss=1.06 (274.3 examples/sec; 0.467 sec/batch)\n",
      "step 2970, loss=0.95 (271.4 examples/sec; 0.472 sec/batch)\n",
      "step 2980, loss=1.08 (272.1 examples/sec; 0.470 sec/batch)\n",
      "step 2990, loss=1.24 (280.7 examples/sec; 0.456 sec/batch)\n"
     ]
    }
   ],
   "source": [
    "# 正式开始训练\n",
    "for step in range(max_steps):\n",
    "    start_time = time.time()\n",
    "    image_batch, label_batch = session.run([images_train, labels_train])\n",
    "    _, loss_value = session.run([train_op, loss], feed_dict={image_holder: image_batch, label_holder: label_batch})\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        examples_per_sec = batch_size / duration\n",
    "        sec_per_batch = float(duration)\n",
    "        \n",
    "        format_str = ('step %d, loss=%.2f (%.1f examples/sec; %.3f sec/batch)')\n",
    "        print(format_str % (step, loss_value, examples_per_sec, sec_per_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试 9984 个examples用时为:9.00916194916秒\n"
     ]
    }
   ],
   "source": [
    "num_examples = 10000\n",
    "import math\n",
    "num_iter = int(math.ceil(num_examples / batch_size))\n",
    "true_count = 0\n",
    "total_sample_count = num_iter * batch_size\n",
    "step = 0\n",
    "test_start_time = time.time()\n",
    "while step < num_iter:\n",
    "    image_batch, label_batch = session.run([images_test, labels_test])\n",
    "    predictions = session.run([top_k_op], feed_dict={image_holder: image_batch, label_holder: label_batch})\n",
    "    true_count += np.sum(predictions)\n",
    "    step += 1\n",
    "end = time.time()\n",
    "print('测试 ' + str(total_sample_count) + ' 个examples用时为:' + str(end - test_start_time) + '秒')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision @ 1 = 0.713\n"
     ]
    }
   ],
   "source": [
    "precision = true_count * 1.0 / total_sample_count\n",
    "print('precision @ 1 = %.3f' % precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
