{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow实现Google Inception Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|层数|类型|kernel尺寸/步长(或注释)|输入尺寸|\n",
    "|:--:|:--:|:--:|:--:|\n",
    "|1| 卷积 | 3 * 3 / 2 | 299 * 299 * 3 |\n",
    "|2| 卷积 | 3 * 3 / 1 | 149 * 149 * 32 |\n",
    "|3| 卷积 | 3 * 3 / 1 | 147 * 147 * 32 |\n",
    "|4| 池化 | 3 * 3 / 2 | 147 * 147 * 64 |\n",
    "|5| 卷积 | 3 * 3 / 1 | 73 * 73 * 64 |\n",
    "|6| 卷积 | 3 * 3 / 2 | 71 * 71 * 80 |\n",
    "|7| 卷积 | 3 * 3 / 1 | 35 * 35 * 192 |\n",
    "|8| Inception模块组 | 3个Inception Module | 35 * 35 * 288 |\n",
    "|9| Inception模块组 | 3个Inception Module | 17 * 17 * 768 |\n",
    "|10| Inception模块组 | 3个Inception Module | 8 * 8 * 1280 |\n",
    "|11| 池化 | 8 * 8 | 8 * 8 * 2048 |\n",
    "|12| 线性 | logits | 1 * 1 * 2048 |\n",
    "|13| Softmax | 分类输出 | 1 * 1 * 1000 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inception V3中设计CNN的思想和Trick:\n",
    "1. Factorization into small convolutions很有效，可以降低参数数量，减轻过拟合，增加网络非线性的表达能力；\n",
    "2. 卷积网络从输入到输出，应该让图片尺寸逐渐减小，输出通道数逐渐增加，即让空间结构化，将空间信息转化为高阶抽象的特征信息；\n",
    "3. Inception Module用多个分支提取不同抽象程度的高阶特征的思路很有效，可以丰富网络的表达能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# author: JiRuiqing\n",
    "# email: ruiqing0706@gmail.com\n",
    "\n",
    "# 导包\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "import math\n",
    "import time\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "# 产生截断的正态分布函数\n",
    "trunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成网络中经常用到的函数的默认参数\n",
    "def inception_v3_arg_scope(weight_decay=0.00004, stddev=0.1, batch_norm_var_collection='moving_vars'):\n",
    "    batch_norm_params = {\n",
    "        'decay': 0.9997,\n",
    "        'epsilon': 0.001,\n",
    "        'updates_collections': tf.GraphKeys.UPDATE_OPS,\n",
    "        'variables_collections': {\n",
    "            'beta': None,\n",
    "            'gamma': None,\n",
    "            'moving_mean': [batch_norm_var_collection],\n",
    "            'moving_variance': [batch_norm_var_collection],\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected], weights_regularizer=slim.l2_regularizer(weight_decay)):\n",
    "        with slim.arg_scope([slim.conv2d], weights_initializer=tf.truncated_normal_initializer(stddev=stddev), activation_fn=tf.nn.relu, normalizer_fn=slim.batch_norm, normalizer_params=batch_norm_params) as sc:\n",
    "            return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义生成Inception V3网络卷积部分的函数\n",
    "def inception_v3_base(inputs, scope=None):\n",
    "    '''\n",
    "    inputs: 输入的tensor\n",
    "    scope: 函数默认参数配置\n",
    "    '''\n",
    "    end_points = {} # 保存某些关键节点的字典供之后使用\n",
    "    \n",
    "    with tf.variable_scope(scope, 'InceptionV3', [inputs]):\n",
    "        with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], # 对三个参数设置默认值\n",
    "                            stride=1, padding='VALID'):\n",
    "\n",
    "            # 定义Inception V3的网络结构。\n",
    "            # 5个卷积层，2个池化层。\n",
    "            \n",
    "            # inputs为输入299 x 299 x 3的tensor\n",
    "            # 输入tensor, 通道数, 卷积核尺寸, 步长stride, padding模式\n",
    "            net = slim.conv2d(inputs, 32, [3, 3], stride=2, scope='Conv2d_1a_3x3')\n",
    "            # \n",
    "            net = slim.conv2d(net, 32, [3, 3], scope='Conv2d_2a_3x3')\n",
    "            net = slim.conv2d(net, 64, [3, 3], padding='SAME', scope='Conv2d_2b_3x3')\n",
    "            net = slim.max_pool2d(net, [3, 3], stride=2, scope='MaxPool_3a_3x3')\n",
    "            net = slim.conv2d(net, 80, [1, 1], scope='Conv2d_3b_1x1')\n",
    "            net = slim.conv2d(net, 192, [3, 3], scope='Conv2d_4a_3x3')\n",
    "            net = slim.max_pool2d(net, [3, 3], stride=2, scope='MaxPoll_5a_3x3')\n",
    "            \n",
    "        with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], stride=1, padding='SAME'):\n",
    "            # 第1个Inception Module —— Mixed_5b\n",
    "            with tf.variable_scope('Mixed_5b'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 48, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 64, [5, 5], scope='Conv2d_0b_5x5')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                    branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0c_3x3')\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "#                     branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                    branch_3 = slim.conv2d(branch_3, 32, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "                \n",
    "            # 第2个Inception Module —— Mixed_5c\n",
    "            with tf.variable_scope('Mixed_5c'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 48, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 64, [5, 5], scope='Conv2d_0c_5x5')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                    branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0c_3x3')\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "                \n",
    "            # 第3个Inception Module —— Mixed_5d\n",
    "            with tf.variable_scope('Mixed_5d'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 48, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 64, [5, 5], scope='Conv2d_0b_5x5')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                    branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0c_3x3')\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "                \n",
    "            # 第4个Inception Module —— Mixed_6b\n",
    "            with tf.variable_scope('Mixed_6a'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 384, [3, 3], stride=2, padding='VALID', scope='Conv2d_1a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "#                     branch_1 = slim.conv2d(branch_1, 96, [3, 7], scope='Conv0b_0b_3x3')\n",
    "                    branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope='Conv2b_0b_3x3')\n",
    "                    branch_1 = slim.conv2d(branch_1, 96, [3, 3], stride=2, padding='VALID', scope='Conv2b_1a_1x1')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID', scope='MaxPool_1a_3x3')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2], 3)\n",
    "                \n",
    "\n",
    "            # 第4个Inception Module —— Mixed_6b\n",
    "            with tf.variable_scope('Mixed_6b'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 128, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 128, [1, 7], scope='Conv0b_0b_1x7')\n",
    "                    branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv0b_0c_7x1')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.conv2d(net, 128, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 128, [7, 1], scope='Conv2d_0b_7x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 128, [1, 7], scope='Conv2d_0c_1x7')\n",
    "                    branch_2 = slim.conv2d(branch_2, 128, [7, 1], scope='Conv2d_0d_7x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 128, [1, 7], scope='Conv2d_0e_1x7')\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "                \n",
    "            # 第3个Inception Module —— Mixed_6c\n",
    "            with tf.variable_scope('Mixed_6c'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 160, [1, 7], scope='Conv0b_0b_1x7')\n",
    "                    branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv0b_0c_7x1')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope='Conv2d_0b_7x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 160, [1, 7], scope='Conv2d_0c_1x7')\n",
    "                    branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope='Conv2d_0d_7x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope='Conv2d_0e_1x7')\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "                \n",
    "            # 第3个Inception Module —— Mixed_6d\n",
    "            with tf.variable_scope('Mixed_6d'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 160, [1, 7], scope='Conv0b_0b_1x7')\n",
    "                    branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv0b_0c_7x1')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope='Conv2d_0b_7x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 160, [1, 7], scope='Conv2d_0c_1x7')\n",
    "                    branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope='Conv2d_0d_7x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope='Conv2d_0e_1x7')\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "                \n",
    "            # 第3个Inception Module —— Mixed_6e\n",
    "            with tf.variable_scope('Mixed_6e'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 192, [1, 7], scope='Conv2b_0b_1x7')\n",
    "                    branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv2b_0c_7x1')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope='Conv2d_0b_7x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope='Conv2d_0c_1x7')\n",
    "                    branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope='Conv2d_0d_7x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope='Conv2d_0e_1x7')\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "            end_points['Mixed_6e'] = net\n",
    "            \n",
    "            # 第?个Inception Module —— Mixed_7a\n",
    "            with tf.variable_scope('Mixed_7a'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_0 = slim.conv2d(branch_0, 320, [3, 3], stride=2, padding='VALID', scope='Conv2d_1a_3x3')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 192, [1, 7], scope='Conv2b_0b_1x7')\n",
    "                    branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv2b_0c_7x7')\n",
    "                    branch_1 = slim.conv2d(branch_1, 192, [3, 3], stride=2, padding='VALID', scope='Conv0b_1a_3x3')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID', scope='MaxPool_1a_3x3')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2], 3)\n",
    "                \n",
    "            # 第?个Inception Module —— Mixed_7b\n",
    "            with tf.variable_scope('Mixed_7b'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 320, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 384, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1 = tf.concat([slim.conv2d(branch_1, 384, [1, 3], scope='Conv0b_0b_1x3'), slim.conv2d(branch_1, 384, [3, 1], scope='Conv0b_0b_3x1')], 3)\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.conv2d(net, 448, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 384, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                    branch_2 = tf.concat([slim.conv2d(branch_2, 384, [1, 3], scope='Conv2d_0c_1x3'), slim.conv2d(branch_2, 384, [3, 1], scope='Conv2d_0d_3x1')], 3)\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "                \n",
    "            # 第?个Inception Module —— Mixed_7c\n",
    "            with tf.variable_scope('Mixed_7c'):\n",
    "                with tf.variable_scope('Branch_0'):\n",
    "                    branch_0 = slim.conv2d(net, 320, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'):\n",
    "                    branch_1 = slim.conv2d(net, 384, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1 = tf.concat([slim.conv2d(branch_1, 384, [1, 3], scope='Conv0b_0b_1x3'), slim.conv2d(branch_1, 384, [3, 1], scope='Conv0b_0c_3x1')], 3)\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.conv2d(net, 448, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 384, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                    branch_2 = tf.concat([slim.conv2d(branch_2, 384, [1, 3], scope='Conv2d_0c_1x3'), slim.conv2d(branch_2, 384, [3, 1], scope='Conv2d_0d_3x1')], 3)\n",
    "                with tf.variable_scope('Branch_3'):\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "            return net, end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_v3(inputs, num_classes=1000, is_training=True, dropout_keep_prob=0.8, prediction_fn=slim.softmax, spatial_squeeze=True, reuse=None, scope='InceptionV3'):\n",
    "    with tf.variable_scope(scope, 'InceptionV3', [inputs, num_classes], reuse=reuse) as scope:\n",
    "        with slim.arg_scope([slim.batch_norm, slim.dropout], is_training=is_training):\n",
    "            net, end_points = inception_v3_base(inputs, scope=scope)\n",
    "            \n",
    "            # Auxiliary Logits\n",
    "            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], stride=1, padding='SAME'):\n",
    "                aux_logits = end_points['Mixed_6e']\n",
    "                with tf.variable_scope('AuxLogits'):\n",
    "                    aux_logits = slim.avg_pool2d(aux_logits, [5, 5], stride=3, padding='VALID', scope='AvgPool_1a_5x5')\n",
    "                    aux_logits = slim.conv2d(aux_logits, 128, [1, 1], scope='Conv2d_1b_1x1')\n",
    "                    aux_logits = slim.conv2d(aux_logits, 768, [5, 5], weights_initializer=trunc_normal(0.01), padding='VALID', scope='Conv2d_2a_5x5')\n",
    "                    aux_logits = slim.conv2d(aux_logits, num_classes, [1, 1], activation_fn=None, normalizer_fn=None, weights_initializer=trunc_normal(0.001), scope='Conv2d_2b_1x1')\n",
    "\n",
    "                    if spatial_squeeze:\n",
    "                        aux_logits = tf.squeeze(aux_logits, [1, 2], name='SpatialSqueeze')\n",
    "                    end_points['AuxLogits'] = aux_logits\n",
    "\n",
    "            # 处理正常的分类预测的逻辑\n",
    "            with tf.variable_scope('Logits'):\n",
    "                net = slim.avg_pool2d(net, [8, 8], padding='VALID', scope='AvgPool_1a_8x8')\n",
    "                net = slim.dropout(net, keep_prob=dropout_keep_prob, scope='Dropout_1b')\n",
    "                end_points['PreLogits'] = net\n",
    "                logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, normalizer_fn=None, scope='Conv2d_1c_1x1')\n",
    "\n",
    "                if spatial_squeeze:\n",
    "                    logits = tf.squeeze(logits, [1, 2], name='SpatialSqueeze')\n",
    "            end_points['Logits'] = logits\n",
    "            end_points['predictions'] = prediction_fn(logits, scope='Predictions')\n",
    "    return logits, end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########评估AlexNet每轮计算时间########\n",
    "def time_tensorflow_run(session, target, info_string):\n",
    "\n",
    "  # Args:\n",
    "  # session:the TensorFlow session to run the computation under.\n",
    "  # target:需要评测的运算算子。\n",
    "  # info_string:测试名称。\n",
    "\n",
    "  num_steps_burn_in = 10 # 先定义预热轮数（头几轮跌代有显存加载、cache命中等问题因此可以跳过，只考量10轮迭代之后的计算时间）\n",
    "  total_duration = 0.0 # 记录总时间\n",
    "  total_duration_squared = 0.0 # 总时间平方和  -----用来后面计算方差\n",
    "  for i in xrange(num_batches + num_steps_burn_in): # 迭代轮数\n",
    "    start_time = time.time() # 记录时间\n",
    "    _ = session.run(target) # 每次迭代通过session.run(target)\n",
    "    duration = time.time() - start_time # \n",
    "    if i >= num_steps_burn_in: \n",
    "      if not i % 10:\n",
    "        print ('%s: step %d, duration = %.3f' %\n",
    "               (datetime.now(), i - num_steps_burn_in, duration))\n",
    "      total_duration += duration  # 累加便于后面计算每轮耗时的均值和标准差\n",
    "      total_duration_squared += duration * duration\n",
    "  mn = total_duration / num_batches # 每轮迭代的平均耗时\n",
    "  vr = total_duration_squared / num_batches - mn * mn \n",
    "  sd = math.sqrt(vr) # 标准差\n",
    "  print ('%s: %s across %d steps, %.3f +/- %.3f sec / batch' %\n",
    "         (datetime.now(), info_string, num_batches, mn, sd))\n",
    "# def time_tensorflow_run(session, target, feed, info_string):\n",
    "#     num_steps_burn_in = 10\n",
    "#     total_duration = 0.0\n",
    "#     total_duration_squared = 0.0\n",
    "#     for i in range(num_batches + num_steps_burn_in):\n",
    "#         start_time = time.time()\n",
    "#         _ = session.run(target, feed_dict=feed)\n",
    "#         duration = time.time() - start_time\n",
    "#         if i >= num_steps_burn_in:\n",
    "#             if not i % 10:\n",
    "#                 print('%s: step %d, duration = %.3f' % (datetime.now(), i - num_steps_burn_in, duration))\n",
    "#             total_duration += duration\n",
    "#             total_duration_squared += duration * duration\n",
    "#     mn = total_duration / num_batches\n",
    "#     vr = total_duration_squared / num_batches - mn * mn\n",
    "#     sd = math.sqrt(vr)\n",
    "#     print('%s: %s across %d steps, %.3f +/- %.3f sec / batch' % (datetime.now(), info_string, num_batches, mn, sd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-23 10:11:17.474431: step 0, duration = 4.621\n",
      "2018-01-23 10:12:03.535323: step 10, duration = 4.574\n",
      "2018-01-23 10:12:49.426170: step 20, duration = 4.484\n",
      "2018-01-23 10:13:33.801644: step 30, duration = 4.411\n",
      "2018-01-23 10:14:17.686072: step 40, duration = 4.354\n",
      "2018-01-23 10:15:01.846925: step 50, duration = 4.597\n",
      "2018-01-23 10:15:46.045500: step 60, duration = 4.646\n",
      "2018-01-23 10:16:42.516468: step 70, duration = 6.116\n",
      "2018-01-23 10:17:29.610056: step 80, duration = 4.884\n",
      "2018-01-23 10:18:15.957490: step 90, duration = 4.397\n",
      "2018-01-23 10:18:56.334417: Forward across 100 steps, 4.635 +/- 0.589 sec / batch\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "height, width = 299, 299\n",
    "inputs = tf.random_uniform((batch_size, height, width, 3))\n",
    "with slim.arg_scope(inception_v3_arg_scope()):\n",
    "    logits, end_points = inception_v3(inputs, is_training=False)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "session = tf.Session()\n",
    "session.run(init)\n",
    "num_batches = 100\n",
    "time_tensorflow_run(session, logits, \"Forward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
